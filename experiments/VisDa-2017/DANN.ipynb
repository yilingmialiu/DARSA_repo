{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "311ff04d-d6a2-4aac-a80e-6395ad74b8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../../')\n",
    "sys.path.append('/datacommons/carlsonlab/yl407/packages')\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c7d1ce0-5be8-401d-b5bf-d9782a8119b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "165356f9-c0a3-43cc-ba93-8f72929e507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#import network\n",
    "#import loss\n",
    "#import pre_process as prep\n",
    "from torch.utils.data import DataLoader\n",
    "#import lr_schedule\n",
    "#import data_list\n",
    "#from data_list import ImageList\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import pdb\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "838dc98b-7a04-47db-be2e-fb43c3f482cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from PIL import Image, ImageOps\n",
    "import numbers\n",
    "import torch\n",
    "\n",
    "class ResizeImage():\n",
    "    def __init__(self, size):\n",
    "        if isinstance(size, int):\n",
    "                self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "    def __call__(self, img):\n",
    "        th, tw = self.size\n",
    "        return img.resize((th, tw))\n",
    "\n",
    "\n",
    "class PlaceCrop(object):\n",
    "    \"\"\"Crops the given PIL.Image at the particular index.\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size of the crop. If size is an\n",
    "            int instead of sequence like (w, h), a square crop (size, size) is\n",
    "            made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, start_x, start_y):\n",
    "        if isinstance(size, int):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "        self.start_x = start_x\n",
    "        self.start_y = start_y\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL.Image): Image to be cropped.\n",
    "        Returns:\n",
    "            PIL.Image: Cropped image.\n",
    "        \"\"\"\n",
    "        th, tw = self.size\n",
    "        return img.crop((self.start_x, self.start_y, self.start_x + tw, self.start_y + th))\n",
    "\n",
    "\n",
    "class ForceFlip(object):\n",
    "    \"\"\"Horizontally flip the given PIL.Image randomly with a probability of 0.5.\"\"\"\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL.Image): Image to be flipped.\n",
    "        Returns:\n",
    "            PIL.Image: Randomly flipped image.\n",
    "        \"\"\"\n",
    "        return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "def image_train(resize_size=256, crop_size=224):\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    return  transforms.Compose([\n",
    "        ResizeImage(resize_size),\n",
    "        transforms.RandomResizedCrop(crop_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "def image_test(resize_size=256, crop_size=224):\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    #ten crops for image when validation, input the data_transforms dictionary\n",
    "    start_first = 0\n",
    "    start_center = (resize_size - crop_size - 1) / 2\n",
    "    start_last = resize_size - crop_size - 1\n",
    "    return transforms.Compose([\n",
    "    ResizeImage(resize_size),\n",
    "    PlaceCrop(crop_size, start_center, start_center),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "  ])\n",
    "\n",
    "def image_test_10crop(resize_size=256, crop_size=224):\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    #ten crops for image when validation, input the data_transforms dictionary\n",
    "    start_first = 0\n",
    "    start_center = (resize_size - crop_size - 1) / 2\n",
    "    start_last = resize_size - crop_size - 1\n",
    "    data_transforms = {}\n",
    "    data_transforms['val0'] = transforms.Compose([\n",
    "        ResizeImage(resize_size),ForceFlip(),\n",
    "        PlaceCrop(crop_size, start_first, start_first),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    data_transforms['val1'] = transforms.Compose([\n",
    "        ResizeImage(resize_size),ForceFlip(),\n",
    "        PlaceCrop(crop_size, start_last, start_last),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    data_transforms['val2'] = transforms.Compose([\n",
    "        ResizeImage(resize_size),ForceFlip(),\n",
    "        PlaceCrop(crop_size, start_last, start_first),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    data_transforms['val3'] = transforms.Compose([\n",
    "        ResizeImage(resize_size),ForceFlip(),\n",
    "        PlaceCrop(crop_size, start_first, start_last),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    data_transforms['val4'] = transforms.Compose([\n",
    "        ResizeImage(resize_size),ForceFlip(),\n",
    "        PlaceCrop(crop_size, start_center, start_center),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    data_transforms['val5'] = transforms.Compose([\n",
    "        ResizeImage(resize_size),\n",
    "        PlaceCrop(crop_size, start_first, start_first),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    data_transforms['val6'] = transforms.Compose([\n",
    "        ResizeImage(resize_size),\n",
    "        PlaceCrop(crop_size, start_last, start_last),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    data_transforms['val7'] = transforms.Compose([\n",
    "        ResizeImage(resize_size),\n",
    "        PlaceCrop(crop_size, start_last, start_first),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    data_transforms['val8'] = transforms.Compose([\n",
    "        ResizeImage(resize_size),\n",
    "        PlaceCrop(crop_size, start_first, start_last),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    data_transforms['val9'] = transforms.Compose([\n",
    "        ResizeImage(resize_size),\n",
    "        PlaceCrop(crop_size, start_center, start_center),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    return data_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fc8a547-0375-4e28-829b-4c1dbfc57937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_lr_scheduler(param_lr, optimizer, iter_num, gamma, power, init_lr=0.001):\n",
    "    \"\"\"Decay learning rate by a factor of 0.1 every lr_decay_epoch epochs.\"\"\"\n",
    "    lr = init_lr * (1 + gamma * iter_num) ** (-power)\n",
    "\n",
    "    i=0\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr * param_lr[i]\n",
    "        i+=1\n",
    "    return optimizer\n",
    "\n",
    "schedule_dict = {\"inv\":inv_lr_scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d663d341-1c1b-4f96-9034-60111e1558d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def EntropyLoss(input_):\n",
    "    mask = input_.ge(0.000001)\n",
    "    mask_out = torch.masked_select(input_, mask)\n",
    "    entropy = -(torch.sum(mask_out * torch.log(mask_out)))\n",
    "    return entropy / float(input_.size(0))\n",
    "\n",
    "def DANN(features, ad_net, grl_layer, use_gpu=True):\n",
    "    ad_out = ad_net(grl_layer(features))\n",
    "    batch_size = ad_out.size(0) // 2\n",
    "    dc_target = Variable(torch.from_numpy(np.array([[1]] * batch_size + [[0]] * batch_size)).float())\n",
    "    if use_gpu:\n",
    "        dc_target = dc_target.cuda()\n",
    "    return nn.BCELoss(ad_out.view(-1), dc_target.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5415636b-601b-44c3-bc09-5fabddf88bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from __future__ import print_function, division\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "def make_dataset(image_list, labels):\n",
    "    if labels:\n",
    "        len_ = len(image_list)\n",
    "        images = [(image_list[i].strip(), labels[i, :]) for i in range(len_)]\n",
    "    else:\n",
    "        if len(image_list[0].split()) > 2:\n",
    "            images = [(val.split()[0], np.array([int(la) for la in val.split()[1:]])) for val in image_list]\n",
    "        else:\n",
    "            images = [(val.split()[0], int(val.split()[1])) for val in image_list]\n",
    "    return images\n",
    "\n",
    "\n",
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert('RGB')\n",
    "\n",
    "\n",
    "def accimage_loader(path):\n",
    "    import accimage\n",
    "    try:\n",
    "        return accimage.Image(path)\n",
    "    except IOError:\n",
    "        # Potentially a decoding problem, fall back to PIL.Image\n",
    "        return pil_loader(path)\n",
    "\n",
    "\n",
    "def default_loader(path):\n",
    "    #from torchvision import get_image_backend\n",
    "    #if get_image_backend() == 'accimage':\n",
    "    #    return accimage_loader(path)\n",
    "    #else:\n",
    "        return pil_loader(path)\n",
    "\n",
    "\n",
    "class ImageList(object):\n",
    "    \"\"\"A generic data loader where the images are arranged in this way: ::\n",
    "        root/dog/xxx.png\n",
    "        root/dog/xxy.png\n",
    "        root/dog/xxz.png\n",
    "        root/cat/123.png\n",
    "        root/cat/nsdf3.png\n",
    "        root/cat/asd932_.png\n",
    "    Args:\n",
    "        root (string): Root directory path.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        loader (callable, optional): A function to load an image given its path.\n",
    "     Attributes:\n",
    "        classes (list): List of the class names.\n",
    "        class_to_idx (dict): Dict with items (class_name, class_index).\n",
    "        imgs (list): List of (image path, class_index) tuples\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_list, labels=None, transform=None, target_transform=None,\n",
    "                 loader=default_loader):\n",
    "        imgs = make_dataset(image_list, labels)\n",
    "        if len(imgs) == 0:\n",
    "            raise(RuntimeError(\"Found 0 images in subfolders of: \" + root + \"\\n\"\n",
    "                               \"Supported image extensions are: \" + \",\".join(IMG_EXTENSIONS)))\n",
    "\n",
    "        self.imgs = imgs\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.loader = loader\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is class_index of the target class.\n",
    "        \"\"\"\n",
    "        path, target = self.imgs[index]\n",
    "        img = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "class ImageValueList(object):\n",
    "    \"\"\"A generic data loader where the images are arranged in this way: ::\n",
    "        root/dog/xxx.png\n",
    "        root/dog/xxy.png\n",
    "        root/dog/xxz.png\n",
    "        root/cat/123.png\n",
    "        root/cat/nsdf3.png\n",
    "        root/cat/asd932_.png\n",
    "    Args:\n",
    "        root (string): Root directory path.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        loader (callable, optional): A function to load an image given its path.\n",
    "     Attributes:\n",
    "        classes (list): List of the class names.\n",
    "        class_to_idx (dict): Dict with items (class_name, class_index).\n",
    "        imgs (list): List of (image path, class_index) tuples\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_list, labels=None, transform=None, target_transform=None,\n",
    "                 loader=default_loader):\n",
    "        imgs = make_dataset(image_list, labels)\n",
    "        if len(imgs) == 0:\n",
    "            raise(RuntimeError(\"Found 0 images in subfolders of: \" + root + \"\\n\"\n",
    "                               \"Supported image extensions are: \" + \",\".join(IMG_EXTENSIONS)))\n",
    "\n",
    "        self.imgs = imgs\n",
    "        self.values = [1.0] * len(imgs)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.loader = loader\n",
    "\n",
    "    def set_values(self, values):\n",
    "        self.values = values\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is class_index of the target class.\n",
    "        \"\"\"\n",
    "        path, target = self.imgs[index]\n",
    "        img = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f311cca2-8cff-4463-9017-c3d62c6a1595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class AdversarialLayer(torch.autograd.Function):\n",
    "    def __init__(self, high_value=1.0):\n",
    "        self.iter_num = 0\n",
    "        self.alpha = 10\n",
    "        self.low = 0.0\n",
    "        self.high = high_value\n",
    "        self.max_iter = 10000.0\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.iter_num += 1\n",
    "        output = input * 1.0\n",
    "        return output\n",
    "\n",
    "    def backward(self, gradOutput):\n",
    "        self.coeff = np.float(2.0 * (self.high - self.low) / (1.0 + np.exp(-self.alpha*self.iter_num / self.max_iter)) - (self.high - self.low) + self.low)\n",
    "        return -self.coeff * gradOutput\n",
    "\n",
    "class SilenceLayer(torch.autograd.Function):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, input):\n",
    "        return input * 1.0\n",
    "    def backward(self, gradOutput):\n",
    "        return 0 * gradOutput\n",
    "\n",
    "# convnet without the last layer\n",
    "class AlexNetFc(nn.Module):\n",
    "    def __init__(self, use_bottleneck=True, bottleneck_dim=256, new_cls=False, class_num=1000):\n",
    "        super(AlexNetFc, self).__init__()\n",
    "        model_alexnet = models.alexnet(pretrained=True)\n",
    "        self.features = model_alexnet.features\n",
    "        self.classifier = nn.Sequential()\n",
    "        for i in range(6):\n",
    "            self.classifier.add_module(\"classifier\"+str(i), model_alexnet.classifier[i])\n",
    "        self.feature_layers = nn.Sequential(self.features, self.classifier)\n",
    "\n",
    "        self.use_bottleneck = use_bottleneck\n",
    "        self.new_cls = new_cls\n",
    "        if new_cls:\n",
    "            if self.use_bottleneck:\n",
    "                self.bottleneck = nn.Linear(4096, bottleneck_dim)\n",
    "                self.bottleneck.weight.data.normal_(0, 0.005)\n",
    "                self.bottleneck.bias.data.fill_(0.0)\n",
    "                self.fc = nn.Linear(bottleneck_dim, class_num)\n",
    "                self.fc.weight.data.normal_(0, 0.01)\n",
    "                self.fc.bias.data.fill_(0.0)\n",
    "                self.__in_features = bottleneck_dim\n",
    "            else:\n",
    "                self.fc = nn.Linear(4096, class_num)\n",
    "                self.fc.weight.data.normal_(0, 0.01)\n",
    "                self.fc.bias.data.fill_(0.0)\n",
    "                self.__in_features = 4096\n",
    "        else:\n",
    "            self.fc = model_alexnet.classifier[6]\n",
    "            self.__in_features = 4096\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        if self.use_bottleneck and self.new_cls:\n",
    "            x = self.bottleneck(x)\n",
    "        y = self.fc(x)\n",
    "        return x, y\n",
    "\n",
    "    def output_num(self):\n",
    "        return self.__in_features\n",
    "\n",
    "\n",
    "resnet_dict = {\"ResNet18\":models.resnet18, \"ResNet34\":models.resnet34, \"ResNet50\":models.resnet50, \"ResNet101\":models.resnet101, \"ResNet152\":models.resnet152}\n",
    "\n",
    "class ResNetFc(nn.Module):\n",
    "    def __init__(self, resnet_name, use_bottleneck=True, bottleneck_dim=256, new_cls=False, class_num=1000):\n",
    "        super(ResNetFc, self).__init__()\n",
    "        model_resnet = resnet_dict[resnet_name](pretrained=True)\n",
    "        self.conv1 = model_resnet.conv1\n",
    "        self.bn1 = model_resnet.bn1\n",
    "        self.relu = model_resnet.relu\n",
    "        self.maxpool = model_resnet.maxpool\n",
    "        self.layer1 = model_resnet.layer1\n",
    "        self.layer2 = model_resnet.layer2\n",
    "        self.layer3 = model_resnet.layer3\n",
    "        self.layer4 = model_resnet.layer4\n",
    "        self.avgpool = model_resnet.avgpool\n",
    "        self.feature_layers = nn.Sequential(self.conv1, self.bn1, self.relu, self.maxpool, \\\n",
    "                                            self.layer1, self.layer2, self.layer3, self.layer4, self.avgpool)\n",
    "\n",
    "        self.use_bottleneck = use_bottleneck\n",
    "        self.new_cls = new_cls\n",
    "        if new_cls:\n",
    "            if self.use_bottleneck:\n",
    "                self.bottleneck = nn.Linear(model_resnet.fc.in_features, bottleneck_dim)\n",
    "                self.bottleneck.weight.data.normal_(0, 0.005)\n",
    "                self.bottleneck.bias.data.fill_(0.0)\n",
    "                self.fc = nn.Linear(bottleneck_dim, class_num)\n",
    "                self.fc.weight.data.normal_(0, 0.01)\n",
    "                self.fc.bias.data.fill_(0.0)\n",
    "                self.__in_features = bottleneck_dim\n",
    "            else:\n",
    "                self.fc = nn.Linear(model_resnet.fc.in_features, class_num)\n",
    "                self.fc.weight.data.normal_(0, 0.01)\n",
    "                self.fc.bias.data.fill_(0.0)\n",
    "                self.__in_features = model_resnet.fc.in_features\n",
    "        else:\n",
    "            self.fc = model_resnet.fc\n",
    "            self.__in_features = model_resnet.fc.in_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        if self.use_bottleneck and self.new_cls:\n",
    "            x = self.bottleneck(x)\n",
    "        y = self.fc(x)\n",
    "        return x, y\n",
    "\n",
    "    def output_num(self):\n",
    "        return self.__in_features\n",
    "\n",
    "vgg_dict = {\"VGG11\":models.vgg11, \"VGG13\":models.vgg13, \"VGG16\":models.vgg16, \"VGG19\":models.vgg19, \"VGG11BN\":models.vgg11_bn, \"VGG13BN\":models.vgg13_bn, \"VGG16BN\":models.vgg16_bn, \"VGG19BN\":models.vgg19_bn} \n",
    "class VGGFc(nn.Module):\n",
    "    def __init__(self, vgg_name, use_bottleneck=True, bottleneck_dim=256, new_cls=False, class_num=1000):\n",
    "        super(VGGFc, self).__init__()\n",
    "        model_vgg = vgg_dict[vgg_name](pretrained=True)\n",
    "        self.features = model_vgg.features\n",
    "        self.classifier = nn.Sequential()\n",
    "        for i in range(6):\n",
    "            self.classifier.add_module(\"classifier\"+str(i), model_vgg.classifier[i])\n",
    "        self.feature_layers = nn.Sequential(self.features, self.classifier)\n",
    "\n",
    "        self.use_bottleneck = use_bottleneck\n",
    "        self.new_cls = new_cls\n",
    "        if new_cls:\n",
    "            if self.use_bottleneck:\n",
    "                self.bottleneck = nn.Linear(4096, bottleneck_dim)\n",
    "                self.bottleneck.weight.data.normal_(0, 0.005)\n",
    "                self.bottleneck.bias.data.fill_(0.0)\n",
    "                self.fc = nn.Linear(bottleneck_dim, class_num)\n",
    "                self.fc.weight.data.normal_(0, 0.01)\n",
    "                self.fc.bias.data.fill_(0.0)\n",
    "                self.__in_features = bottleneck_dim\n",
    "            else:\n",
    "                self.fc = nn.Linear(4096, class_num)\n",
    "                self.fc.weight.data.normal_(0, 0.01)\n",
    "                self.fc.bias.data.fill_(0.0)\n",
    "                self.__in_features = 4096\n",
    "        else:\n",
    "            self.fc = model_vgg.classifier[6]\n",
    "            self.__in_features = 4096\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.features(x)\n",
    "            x = x.view(x.size(0), 25088)\n",
    "            x = self.classifier(x)\n",
    "            if self.use_bottleneck and self.new_cls:\n",
    "                x = self.bottleneck(x)\n",
    "            y = self.fc(x)\n",
    "            return x, y\n",
    "\n",
    "        def output_num(self):\n",
    "            return self.__in_features\n",
    "\n",
    "class AdversarialNetwork(nn.Module):\n",
    "    def __init__(self, in_feature):\n",
    "        super(AdversarialNetwork, self).__init__()\n",
    "        self.ad_layer1 = nn.Linear(in_feature, 1024)\n",
    "        self.ad_layer2 = nn.Linear(1024,1024)\n",
    "        self.ad_layer3 = nn.Linear(1024, 1)\n",
    "        self.ad_layer1.weight.data.normal_(0, 0.01)\n",
    "        self.ad_layer2.weight.data.normal_(0, 0.01)\n",
    "        self.ad_layer3.weight.data.normal_(0, 0.3)\n",
    "        self.ad_layer1.bias.data.fill_(0.0)\n",
    "        self.ad_layer2.bias.data.fill_(0.0)\n",
    "        self.ad_layer3.bias.data.fill_(0.0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ad_layer1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.ad_layer2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.ad_layer3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def output_num(self):\n",
    "        return 1\n",
    "\n",
    "class SmallAdversarialNetwork(nn.Module):\n",
    "    def __init__(self, in_feature):\n",
    "        super(SmallAdversarialNetwork, self).__init__()\n",
    "        self.ad_layer1 = nn.Linear(in_feature, 256)\n",
    "        self.ad_layer2 = nn.Linear(256, 1)\n",
    "        self.ad_layer1.weight.data.normal_(0, 0.01)\n",
    "        self.ad_layer2.weight.data.normal_(0, 0.01)\n",
    "        self.ad_layer1.bias.data.fill_(0.0)\n",
    "        self.ad_layer2.bias.data.fill_(0.0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ad_layer1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.ad_layer2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def output_num(self):\n",
    "        return 1\n",
    "\n",
    "class LittleAdversarialNetwork(nn.Module):\n",
    "    def __init__(self, in_feature):\n",
    "        super(LittleAdversarialNetwork, self).__init__()\n",
    "        self.ad_layer1 = nn.Linear(in_feature, 1)\n",
    "        self.ad_layer1.weight.data.normal_(0, 0.01)\n",
    "        self.ad_layer1.bias.data.fill_(0.0)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.ad_layer1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def output_num(self):\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf6519b-0b7b-47c4-9b5b-81cad927a5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the Office-Home dataset\n",
    "data_path = \"/datacommons/carlsonlab/yl407/OfficeHomeDataset_10072016\"\n",
    "\n",
    "# Define the image transformations to be applied\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize the image to 224x224 pixels\n",
    "    transforms.ToTensor(),         # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "])\n",
    "\n",
    "# Load the Office-Home dataset\n",
    "train_dataset = datasets.ImageFolder(root=data_path+\"/train\", transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=data_path+\"/test\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4ead85-6190-49e7-8578-60c28bf10357",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
