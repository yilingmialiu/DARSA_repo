{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement DANN on MNIST   with a single hidden layer\n",
    "# and a single hidden layer for the target domain\n",
    "# The target domain is the MNIST digits\n",
    "# The source domain is the USPS digits\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/datacommons/carlsonlab/yl407/packages')\n",
    "sys.path.insert(0, '../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Failed to load PyTorch C extensions:\n    It appears that PyTorch has loaded the `torch/_C` folder\n    of the PyTorch repository rather than the C extensions which\n    are expected in the `torch._C` namespace. This can occur when\n    using the `install` workflow. e.g.\n        $ python setup.py install && python -c \"import torch\"\n\n    This error can generally be solved using the `develop` workflow\n        $ python setup.py develop && python -c \"import torch\"  # This should succeed\n    or by running Python from a different directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "File \u001b[0;32m/datacommons/carlsonlab/yl407/packages/torch/__init__.py:209\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;66;03m# The __file__ check only works for Python 3.7 and above.\u001b[39;00m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m7\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m _C_for_compiled_check\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__file__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(textwrap\u001b[38;5;241m.\u001b[39mdedent(\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124m            Failed to load PyTorch C extensions:\u001b[39m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;124m                It appears that PyTorch has loaded the `torch/_C` folder\u001b[39m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;124m                of the PyTorch repository rather than the C extensions which\u001b[39m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;124m                are expected in the `torch._C` namespace. This can occur when\u001b[39m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;124m                using the `install` workflow. e.g.\u001b[39m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;124m                    $ python setup.py install && python -c \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimport torch\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    216\u001b[0m \n\u001b[1;32m    217\u001b[0m \u001b[38;5;124m                This error can generally be solved using the `develop` workflow\u001b[39m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124m                    $ python setup.py develop && python -c \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimport torch\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  # This should succeed\u001b[39m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124m                or by running Python from a different directory.\u001b[39m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;124m'''\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(_C):\n",
      "\u001b[0;31mImportError\u001b[0m: Failed to load PyTorch C extensions:\n    It appears that PyTorch has loaded the `torch/_C` folder\n    of the PyTorch repository rather than the C extensions which\n    are expected in the `torch._C` namespace. This can occur when\n    using the `install` workflow. e.g.\n        $ python setup.py install && python -c \"import torch\"\n\n    This error can generally be solved using the `develop` workflow\n        $ python setup.py develop && python -c \"import torch\"  # This should succeed\n    or by running Python from a different directory."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "torch.manual_seed(123)\n",
    "np.random.seed(123)\n",
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.9 from \"/hpc/group/carlsonlab/yl407/miniconda3/envs/env/bin/python\"\n  * The NumPy version is: \"1.23.4\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: No module named 'numpy.core._multiarray_umath'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/datacommons/carlsonlab/yl407/packages/numpy/core/__init__.py:23\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multiarray\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/datacommons/carlsonlab/yl407/packages/numpy/core/multiarray.py:10\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m overrides\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _multiarray_umath\n",
      "File \u001b[0;32m/datacommons/carlsonlab/yl407/packages/numpy/core/overrides.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_multiarray_umath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     add_docstring, implement_array_function, _get_implementing_args)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inspect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getargspec\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy.core._multiarray_umath'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[0;32m/datacommons/carlsonlab/yl407/packages/numpy/__init__.py:140\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# Allow distributors to run custom init code\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _distributor_init\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n",
      "File \u001b[0;32m/datacommons/carlsonlab/yl407/packages/numpy/core/__init__.py:49\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[38;5;124mIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m \u001b[38;5;241m%\u001b[39m (sys\u001b[38;5;241m.\u001b[39mversion_info[\u001b[38;5;241m0\u001b[39m], sys\u001b[38;5;241m.\u001b[39mversion_info[\u001b[38;5;241m1\u001b[39m], sys\u001b[38;5;241m.\u001b[39mexecutable,\n\u001b[1;32m     48\u001b[0m         __version__, exc)\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m envkey \u001b[38;5;129;01min\u001b[39;00m env_added:\n",
      "\u001b[0;31mImportError\u001b[0m: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.9 from \"/hpc/group/carlsonlab/yl407/miniconda3/envs/env/bin/python\"\n  * The NumPy version is: \"1.23.4\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: No module named 'numpy.core._multiarray_umath'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a histogram of x_source\n",
    "def plot_hist(x_source, x_target, title=''):    \n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title(title)\n",
    "    plt.hist(x_source, bins=100, alpha=0.5, label='source')\n",
    "    plt.hist(x_target, bins=100, alpha=0.5, label='target')\n",
    "    plt.legend(fontsize=20)\n",
    "    plt.savefig('moti_after_right.png',dpi=500,bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making synthetic data\n",
    "N=10000\n",
    "# Comprise x_source from a mixture of two Gaussians at -1.5 and 1.5 with weights .70 and .3\n",
    "x_source=np.concatenate((np.random.randn(round(.7*N),1)-1.5,np.random.randn(round(.3*N),1)+1.5),axis=0)\n",
    "# Comprise x_target from a mixture of two Gaussians at -1.5 and 1.5 with weights .25 and .75\n",
    "x_target=np.concatenate((np.random.randn(round(.3*N),1)-1.4,np.random.randn(round(.7*N),1)+1.6),axis=0)\n",
    "    \n",
    "plot_hist(x_source,x_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_source_left = x_source[x_source<=0,]\n",
    "x_target_left = x_target[x_target<=0,]\n",
    "plot_hist(x_source_left,x_target_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geomloss import SamplesLoss\n",
    "# Define a Sinkhorn (~Wasserstein) loss between sampled measures\n",
    "loss = SamplesLoss(loss=\"sinkhorn\", p=1, blur=.05)\n",
    "loss(torch.tensor(x_source),torch.tensor(x_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(torch.tensor(x_source[x_source<=0,].reshape(-1,1)),torch.tensor(x_target[x_target<=0,]).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(torch.tensor(x_source[x_source>0,].reshape(-1,1)),torch.tensor(x_target[x_target>0,]).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data to torch tensors\n",
    "x_source=torch.from_numpy(x_source).float().to(device)\n",
    "x_target=torch.from_numpy(x_target).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_source = torch.ones_like(x_source)\n",
    "y_source[x_source<=0] = 0\n",
    "y_target = torch.ones_like(x_target)\n",
    "y_target[x_target<=0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make datasets\n",
    "source_dataset = torch.utils.data.TensorDataset(x_source, y_source)\n",
    "target_dataset = torch.utils.data.TensorDataset(x_target, y_target)\n",
    "# make data loaders\n",
    "source_loader = torch.utils.data.DataLoader(source_dataset, batch_size=10000, shuffle=False)\n",
    "target_loader = torch.utils.data.DataLoader(target_dataset, batch_size=10000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_requires_grad(model, requires_grad=True):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1, 2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.classifier(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feature_source = Net().to(device)\n",
    "model_feature_target = Net().to(device)\n",
    "model_clf = Classifier().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "feature_source_optim = torch.optim.Adam(model_feature_source.parameters(), lr=lr) \n",
    "feature_target_optim = torch.optim.Adam(model_feature_target.parameters(), lr=lr) \n",
    "clf_optim = torch.optim.Adam(model_clf.parameters(), lr=lr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "clf_criterion_unweight = nn.CrossEntropyLoss()\n",
    "w1_loss = SamplesLoss(loss=\"sinkhorn\", p=1, blur=.05)\n",
    "soft_f = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##initialize clustering weights\n",
    "K = 2\n",
    "w_src = torch.ones(K,1,requires_grad=False).divide(K).to(device)\n",
    "w_tgt = torch.ones(K,1,requires_grad=False).divide(K).to(device)\n",
    "w_imp = torch.ones(K,1,requires_grad=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##initialize losses\n",
    "mean_w1_loss_all = []\n",
    "mean_clf_loss_all = []\n",
    "mean_unweighted_clf_loss_all = []\n",
    "mean_centroid_loss_all = []\n",
    "mean_sntg_loss_all = []\n",
    "mean_accuracy_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "lamb_clf = 1 #0.01\n",
    "lamb_wd = 1\n",
    "weight_update = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs+1):\n",
    "    source_batch_iterator = iter(source_loader)\n",
    "    target_batch_iterator = iter(target_loader)\n",
    "    len_dataloader = min(len(source_loader), len(target_loader))\n",
    "\n",
    "    total_unweight_clf_loss = 0\n",
    "    total_clf_loss = 0\n",
    "    total_centroid_loss = 0\n",
    "    total_sntg_loss = 0\n",
    "    total_w1_loss = 0\n",
    "\n",
    "\n",
    "    for i in range(len_dataloader):\n",
    "\n",
    "        data_source = source_batch_iterator.next()\n",
    "        source_x, source_y = data_source\n",
    "        data_target = target_batch_iterator.next()\n",
    "        target_x, target_y = data_target\n",
    "        source_x, target_x = source_x.to(device), target_x.to(device)\n",
    "\n",
    "        set_requires_grad(model_feature_source, requires_grad=True)\n",
    "        set_requires_grad(model_feature_target, requires_grad=True)\n",
    "        set_requires_grad(model_clf, requires_grad=True)\n",
    "\n",
    "        ##extract latent features\n",
    "        source_y = source_y.to(torch.int64).to(device)\n",
    "        source_y = torch.squeeze(source_y)\n",
    "        target_y = torch.squeeze(target_y)\n",
    "        source_feature = model_feature_source(source_x)\n",
    "        target_feature = model_feature_target(target_x)\n",
    "        target_feature_2 = model_feature_target(target_x)\n",
    "\n",
    "        ##unweighted classification loss\n",
    "        source_preds = model_clf(source_feature)\n",
    "        clf_loss_unweight = clf_criterion(source_preds, source_y)\n",
    "        report_clf_loss_unweight = clf_criterion_unweight(source_preds, source_y)\n",
    "\n",
    "        ##get clustering information\n",
    "        source_y = source_y.to(torch.int64).to(device)\n",
    "        cluster_s = F.one_hot(source_y, num_classes=K).float()\n",
    "        target_y = target_y.to(torch.int64).to(device)\n",
    "        cluster_t = F.one_hot(target_y, num_classes=K).float()\n",
    "\n",
    "        ##weighted classification loss\n",
    "        weighted_clf_err = cluster_s * clf_loss_unweight.reshape(-1,1)\n",
    "        expected_clf_err = torch.mean(weighted_clf_err, dim=0)\n",
    "        clf_loss = torch.sum(expected_clf_err.reshape(K,1) * w_imp)\n",
    "\n",
    "        ##weighted domain invariant loss   \n",
    "        wasserstein_distance = 0\n",
    "        for cluster_id in range(K):\n",
    "            if torch.sum(target_y==cluster_id)!=0:\n",
    "                wasserstein_distance += w_tgt[cluster_id]*w1_loss(source_feature[source_y==cluster_id,],\\\n",
    "                                         target_feature[target_y==cluster_id,]) \n",
    "\n",
    "        \n",
    "        \n",
    "        print(\"wasserstein_distance\",wasserstein_distance)\n",
    "        print(\"clf_loss\",clf_loss)\n",
    "        loss = lamb_clf*clf_loss + lamb_wd * wasserstein_distance\n",
    "\n",
    "        #update weights\n",
    "        with torch.no_grad():\n",
    "            w_src_batch = cluster_s.mean(dim=0) \n",
    "            w_tgt_batch = cluster_t.mean(dim=0)\n",
    "            w_src = w_src * (1 - weight_update) + w_src_batch.reshape(K,1) * weight_update\n",
    "            w_tgt = w_tgt * (1 - weight_update) + w_tgt_batch.reshape(K,1) * weight_update\n",
    "            w_imp = w_tgt/w_src\n",
    "\n",
    "\n",
    "        #backprop feature extraction+classifier\n",
    "        feature_source_optim.zero_grad()\n",
    "        feature_target_optim.zero_grad()\n",
    "        clf_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        feature_source_optim.step()\n",
    "        feature_target_optim.step()\n",
    "        clf_optim.step()\n",
    "\n",
    "        total_w1_loss += wasserstein_distance.item()\n",
    "        total_unweight_clf_loss += report_clf_loss_unweight.item()\n",
    "        total_clf_loss += clf_loss.item()\n",
    "\n",
    "\n",
    "    mean_clf_loss = total_clf_loss/(len_dataloader)\n",
    "    mean_unweighted_clf_loss = total_unweight_clf_loss/(len_dataloader)\n",
    "    mean_w1_loss = total_w1_loss/(len_dataloader)\n",
    "\n",
    "\n",
    "    mean_clf_loss_all.append(mean_clf_loss)\n",
    "    mean_unweighted_clf_loss_all.append(mean_unweighted_clf_loss)\n",
    "    mean_w1_loss_all.append(mean_w1_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_requires_grad(model_feature_source, requires_grad=False)\n",
    "set_requires_grad(model_feature_target, requires_grad=False)\n",
    "set_requires_grad(model_clf, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_source_feature = model_feature_source(x_source)\n",
    "x_target_feature = model_feature_target(x_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(x_source_feature.detach().numpy().flatten()-1.3,x_target_feature.detach().numpy().flatten()+0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_source = x_source_feature.detach().numpy().flatten()-1.3\n",
    "x_target = x_target_feature.detach().numpy().flatten()+0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_loss(torch.tensor(x_source[x_source<=0,]).reshape(-1,1),torch.tensor(x_target[x_target<=0,]).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_loss(torch.tensor(x_source[x_source>0,]).reshape(-1,1),torch.tensor(x_target[x_target>0,]).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_source_left = x_source[x_source<=0,]\n",
    "x_target_left = x_target[x_target<=0,]\n",
    "plot_hist(x_source_left,x_target_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_source_right = x_source[x_source>0,]\n",
    "x_target_right = x_target[x_target>0,]\n",
    "plot_hist(x_source_right,x_target_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "abb0044033f5443d23454735092ad04a737aec3edd5c23b3532dae24ba1c4981"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
